{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFLow-RedesNeuronales.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luctiz/Grupo43-TP2/blob/master/TensorFLow_RedesNeuronales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGOM1TUiKNdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.data import Dataset\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.1f}'.format\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xsotBt9cPkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test =  pd.read_csv(\"https://metadata.fundacionsadosky.org.ar/media/navent/test.csv\", sep=\",\",parse_dates = ['fecha'])\n",
        "train = pd.read_csv(\"https://metadata.fundacionsadosky.org.ar/media/navent/train.csv\", sep=\",\",parse_dates = ['fecha'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfK0coAkKH_V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8f767cae-021a-4bfe-be13-59fca512e33f"
      },
      "source": [
        "!pip install category_encoders\n"
      ],
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.25.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.21.3)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.17.3)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.3.1)\n",
            "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.10.1)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (0.14.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.1->category_encoders) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_3F-mrpTCYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A los nulls los relleno con el promedio de metroscubiertos de todas las publicaciones\n",
        "train['metroscubiertos']=train['metroscubiertos'].fillna(round(train['metroscubiertos'].mean()))\n",
        "test['metroscubiertos']=test['metroscubiertos'].fillna(round(test['metroscubiertos'].mean()))\n",
        "#Luego convierto metroscubiertos a  np.int16\n",
        "train['metroscubiertos']=train['metroscubiertos'].astype(np.int16)\n",
        "test['metroscubiertos']=test['metroscubiertos'].astype(np.int16)\n",
        "\n",
        "#A los nulls los relleno con el promedio de metrostotales de todas las publicaciones\n",
        "train['metrostotales']=train['metrostotales'].fillna(round(train['metrostotales'].mean()))\n",
        "test['metrostotales']=test['metrostotales'].fillna(round(test['metrostotales'].mean()))\n",
        "#Luego convierto metrostotales a  np.int16\n",
        "train['metrostotales']=train['metrostotales'].astype(np.int16)\n",
        "test['metrostotales']=test['metrostotales'].astype(np.int16)\n",
        "\n",
        "#El precio promedio de las publicaciones que no tienen datos de cant de habitaciones esta entre el precio\n",
        "#promedio de las que tienen 2 y 3 habitaciones\n",
        "#Entonces voy a asumir que tienen 3 habitaciones, ya que es el valor al cual el precio mas se les asemeja.\n",
        "train['habitaciones']=train['habitaciones'].fillna(3)\n",
        "test['habitaciones']=test['habitaciones'].fillna(3)\n",
        "#Luego convierto a np.int8\n",
        "train['habitaciones']=train['habitaciones'].astype(np.int8)\n",
        "test['habitaciones']=test['habitaciones'].astype(np.int8)\n",
        "\n",
        "#A los que no tienen datos de antiguedad los relleno con la antiguedad promedio (es 8)\n",
        "train['antiguedad']=train['antiguedad'].fillna(round(train['antiguedad'].mean()))\n",
        "test['antiguedad']=test['antiguedad'].fillna(round(train['antiguedad'].mean()))\n",
        "#Luego convierto a np.uint8\n",
        "train['antiguedad']=train['antiguedad'].astype(np.uint8)\n",
        "test['antiguedad']=test['antiguedad'].astype(np.uint8)\n",
        "\n",
        "#El precio promedio de los que tienen NULL en garage es parecido al de los que tienen 2.\n",
        "#Relleno los nulls de esta columna con un 2.\n",
        "train['garages']=train['garages'].fillna(2)\n",
        "test['garages']=test['garages'].fillna(2)\n",
        "#convierto a np.int8\n",
        "train['garages']=train['garages'].astype(np.int8)\n",
        "test['garages']=test['garages'].astype(np.int8)\n",
        "\n",
        "#El precio promedio de los que tienen NULL en banos es mas parecido al de los que tienen 2.\n",
        "#Relleno los nulls de esta columna con un 2.\n",
        "train['banos']=train['banos'].fillna(2)\n",
        "test['banos']=test['banos'].fillna(2)\n",
        "#convierto a np.int8\n",
        "train['banos']=train['banos'].astype(np.int8)\n",
        "test['banos']=test['banos'].astype(np.int8)\n",
        "\n",
        "train['gimnasio']=train['gimnasio'].astype(np.int8)\n",
        "train['usosmultiples']=train['usosmultiples'].astype(np.int8)\n",
        "train['piscina']=train['piscina'].astype(np.int8)\n",
        "train['escuelascercanas']=train['escuelascercanas'].astype(np.int8)\n",
        "train['centroscomercialescercanos']=train['centroscomercialescercanos'].astype(np.int8)\n",
        "test['gimnasio']=test['gimnasio'].astype(np.int8)\n",
        "test['usosmultiples']=test['usosmultiples'].astype(np.int8)\n",
        "test['piscina']=test['piscina'].astype(np.int8)\n",
        "test['escuelascercanas']=test['escuelascercanas'].astype(np.int8)\n",
        "test['centroscomercialescercanos']=test['centroscomercialescercanos'].astype(np.int8)\n",
        "\n",
        "#Creo categoria para los que no tienen categoria en provincia:\n",
        "train['provincia']=train['provincia'].fillna('N/A')\n",
        "test['provincia']=test['provincia'].fillna('N/A')\n",
        "#Creo categoria para los que no tienen categoria en ciudad:\n",
        "train['ciudad']=train['ciudad'].fillna('N/A')\n",
        "test['ciudad']=test['ciudad'].fillna('N/A')\n",
        "#Relleno los nulls en direccion con un - .\n",
        "train['direccion']=train['direccion'].fillna('-')\n",
        "test['direccion']=test['direccion'].fillna('-')\n",
        "#Creo categoria para los que no tienen categoria en tipodepropiedad:\n",
        "train['tipodepropiedad']=train['tipodepropiedad'].fillna('N/A')\n",
        "test['tipodepropiedad']=test['tipodepropiedad'].fillna('N/A')\n",
        "\n",
        "train['titulo']=train['titulo'].fillna(' ')\n",
        "train['descripcion']=train['descripcion'].fillna(' ')\n",
        "test['titulo']=test['titulo'].fillna(' ')\n",
        "test['descripcion']=test['descripcion'].fillna(' ')\n",
        "\n",
        "train['precio']=train['precio'].astype(np.int32)\n",
        "train['id']=train['id'].astype(np.int32)\n",
        "test['id']=test['id'].astype(np.int32)\n",
        "\n",
        "train=train.fillna(0)\n",
        "test=test.fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRp8aUlLF_0Y",
        "colab_type": "text"
      },
      "source": [
        "PROBANDO CON TODOS LOS FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u05KNbgXDzj2",
        "colab": {}
      },
      "source": [
        "#Voy a usar OneHotEncoder para convertir la columna categorica tipodepropiedad en dummies.\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder()\n",
        "dummies = enc.fit_transform(train['tipodepropiedad'].values.reshape(-1, 1)).toarray()\n",
        "dummies = pd.DataFrame(dummies)\n",
        "dummies_test = enc.transform(test['tipodepropiedad'].values.reshape(-1, 1)).toarray()\n",
        "dummies_test = pd.DataFrame(dummies_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j0-4yKs1Dzj5",
        "colab": {}
      },
      "source": [
        "dummies.columns=enc.categories_[0].tolist()\n",
        "dummies=dummies.add_prefix('prop_')\n",
        "dummies=dummies.astype(np.int8)\n",
        "\n",
        "dummies_test.columns=enc.categories_[0].tolist()\n",
        "dummies_test=dummies_test.add_prefix('prop_')\n",
        "dummies_test=dummies_test.astype(np.int8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b36w44rfDzkH",
        "colab": {}
      },
      "source": [
        "#Voy a usar OneHotEncoder para convertir la columna categorica provincia en dummies.\n",
        "enc_prov = OneHotEncoder()\n",
        "dummies_prov = enc_prov.fit_transform(train['provincia'].values.reshape(-1, 1)).toarray()\n",
        "dummies_prov = pd.DataFrame(dummies_prov)\n",
        "\n",
        "dummies_prov_test = enc_prov.transform(test['provincia'].values.reshape(-1, 1)).toarray()\n",
        "dummies_prov_test = pd.DataFrame(dummies_prov_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RwMbayQLDzkO",
        "colab": {}
      },
      "source": [
        "dummies_prov.columns=enc_prov.categories_[0].tolist()\n",
        "dummies_prov=dummies_prov.add_prefix('prov_')\n",
        "dummies_prov=dummies_prov.astype(np.int8)\n",
        "\n",
        "dummies_prov_test.columns=enc_prov.categories_[0].tolist()\n",
        "dummies_prov_test=dummies_prov_test.add_prefix('prov_')\n",
        "dummies_prov_test=dummies_prov_test.astype(np.int8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x3snEpJ0DzkZ",
        "colab": {}
      },
      "source": [
        "#Mismo procedimiento que el anterior pero para ciudades\n",
        "enc_ciud = OneHotEncoder(handle_unknown='ignore')\n",
        "dummies_ciud = enc_ciud.fit_transform(train['ciudad'].values.reshape(-1, 1)).toarray()\n",
        "dummies_ciud = pd.DataFrame(dummies_ciud)\n",
        "\n",
        "dummies_ciud_test = enc_ciud.transform(test['ciudad'].values.reshape(-1, 1)).toarray()\n",
        "dummies_ciud_test = pd.DataFrame(dummies_ciud_test)\n",
        "\n",
        "dummies_ciud.columns=enc_ciud.categories_[0].tolist()\n",
        "dummies_ciud=dummies_ciud.add_prefix('ciud_')\n",
        "dummies_ciud=dummies_ciud.astype(np.int8)\n",
        "\n",
        "dummies_ciud_test.columns=enc_ciud.categories_[0].tolist()\n",
        "dummies_ciud_test=dummies_ciud_test.add_prefix('ciud_')\n",
        "dummies_ciud_test=dummies_ciud_test.astype(np.int8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3GemqKzdDzkf",
        "colab": {}
      },
      "source": [
        "#Califico las fechas de acuerdo a la diferencia entre la fecha actual y la fecha mínima sobre el máximo de dias\n",
        "fecha_min = train.fecha.min()\n",
        "fecha_max = train.fecha.max()\n",
        "delta_max_days = (fecha_max - fecha_min).days\n",
        "def calificar_fecha(fecha):\n",
        "    delta = fecha - fecha_min\n",
        "    return delta.days/delta_max_days"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wi0CoPv5Dzkj",
        "colab": {}
      },
      "source": [
        "train[\"puntaje_por_fecha\"] = train['fecha'].transform(lambda x: calificar_fecha(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xSBgj_1pDzkm",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "#Set de test\n",
        "fecha_min = test.fecha.min()\n",
        "fecha_max = test.fecha.max()\n",
        "delta_max_days = (fecha_max - fecha_min).days\n",
        "test[\"puntaje_por_fecha\"] = test['fecha'].transform(lambda x: calificar_fecha(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tJo0bmcLDzks",
        "colab": {}
      },
      "source": [
        "#Paso todas las descripciones a minúscula para contar las palabras con mas precision\n",
        "train['descripcion']=train['descripcion'].apply(lambda x: x.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CkfO4qykDzkw",
        "colab": {}
      },
      "source": [
        "Palabras_mayor_precio=['cuarto','vestidor','terraza','family','vista','bodega',\\\n",
        "                       'salón','estudio','jardín','room','jacuzzi','lujo','antecomedor',\\\n",
        "                       'tv','fiestas','juegos','estacionamiento','gimnasio','nado',\\\n",
        "                       'lugares','doble','desayunador','servicio','chimenea','visita',\\\n",
        "                       'acabado','espacio','salon','alberca','garden','spa']\n",
        "Palabras_menor_precio=['minutos','acept','boiler','reja','credito','transporte'\\\n",
        "                      ,'cochera','contado','escuela','inf','fovissste','cerca',\\\n",
        "                      'bancario','protecciones','patio','infonavit']\n",
        "Palabras=Palabras_mayor_precio + Palabras_menor_precio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4L-TA8esDzk6",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "dummies_palabras_descr=pd.DataFrame()\n",
        "#Por cada palabra en la lista genero una columna que diga si la descripcion contenia esa palabra o no\n",
        "for palabra in Palabras:\n",
        "    dummies_palabras_descr[palabra]=train['descripcion'].apply(lambda x: palabra in x).astype(np.int8)\n",
        "#Renombro estas columnas agregandoles un prefijo\n",
        "dummies_palabras_descr=dummies_palabras_descr.add_prefix('desc_')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hh33Qj71DzlG",
        "colab": {}
      },
      "source": [
        "dummies_palabras_descr_test=pd.DataFrame()\n",
        "#Hago lo mismo para el set de test\n",
        "for palabra in Palabras:\n",
        "    dummies_palabras_descr_test[palabra]=test['descripcion'].apply(lambda x: palabra in x).astype(np.int8)\n",
        "dummies_palabras_descr_test=dummies_palabras_descr_test.add_prefix('desc_')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DOu14SkTEF4B",
        "colab": {}
      },
      "source": [
        "#Paso todas los titulos a minúscula para contar las palabras con mas precision\n",
        "train['titulo']=train['titulo'].apply(lambda x: x.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R0_0OZhBEF4N",
        "colab": {}
      },
      "source": [
        "Palabras_mayor_precio=['polanco','loma','herradura','condesa','bosque','country',\\\n",
        "                       'hermosa','vista','residencia','condominio','golf','lujo','huixquilucan',\\\n",
        "                      ]\n",
        "Palabras_menor_precio=['terreno','fracc','casa','villa','remate','recamaras'\\\n",
        "                      ,'ecatepec','cerca','coacalco','izcalli','planta','lote',\\\n",
        "                      'bonita','cautitlan']\n",
        "Palabras=Palabras_mayor_precio + Palabras_menor_precio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u3rsiqymEF4Y",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "dummies_palabras_titulo=pd.DataFrame()\n",
        "#Por cada palabra en la lista genero una columna que diga si la descripcion contenia esa palabra o no\n",
        "for palabra in Palabras:\n",
        "    dummies_palabras_titulo[palabra]=train['titulo'].apply(lambda x: palabra in x).astype(np.int8)\n",
        "#Renombro estas columnas agregandoles un prefijo\n",
        "dummies_palabras_titulo=dummies_palabras_titulo.add_prefix('titl_')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nw38p6uYEF4h",
        "colab": {}
      },
      "source": [
        "dummies_palabras_titulo_test=pd.DataFrame()\n",
        "#Hago lo mismo para el set de test\n",
        "for palabra in Palabras:\n",
        "    dummies_palabras_titulo_test[palabra]=test['titulo'].apply(lambda x: palabra in x).astype(np.int8)\n",
        "dummies_palabras_titulo_test=dummies_palabras_titulo_test.add_prefix('titl_')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnLrK1gUFYqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['dia'] = train['fecha'].dt.day\n",
        "train['mes'] = train['fecha'].dt.month\n",
        "train['año'] = train['fecha'].dt.year"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyS2RuMWFYqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test['dia'] = test['fecha'].dt.day\n",
        "test['mes'] = test['fecha'].dt.month\n",
        "test['año'] = test['fecha'].dt.year"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kQ4veUePZ3Br",
        "colab": {}
      },
      "source": [
        "train['publicaciones_por_mes'] = train[['año','mes', 'id']].groupby(['año','mes']).transform(lambda x: x.count())\n",
        "train['publicaciones_por_mes'] = train['publicaciones_por_mes'].fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tSxbzdKxKYN6",
        "colab": {}
      },
      "source": [
        "test['publicaciones_por_mes'] = test[['año','mes', 'id']].groupby(['año','mes']).transform(lambda x: x.count())\n",
        "test['publicaciones_por_mes'] = test['publicaciones_por_mes'].fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zChv4LmcvvrZ",
        "colab": {}
      },
      "source": [
        "train['publicaciones_por_dia'] = train[['dia','año','mes', 'id']].groupby(['dia','año','mes']).transform(lambda x: x.count()).fillna(0)\n",
        "test['publicaciones_por_dia'] = test[['dia','año','mes', 'id']].groupby(['dia','año','mes']).transform(lambda x: x.count()).fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UcHAz62k5tNO",
        "colab": {}
      },
      "source": [
        "train['publicaciones_por_año'] = train[['año','id']].groupby(['año']).transform(lambda x: x.count()).fillna(0)\n",
        "test['publicaciones_por_año'] = test[['año','id']].groupby(['año']).transform(lambda x: x.count()).fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RtcXov22rju2",
        "colab": {}
      },
      "source": [
        "dummies_palabras=dummies_palabras_descr.join(dummies_palabras_titulo)\n",
        "dummies_palabras_test=dummies_palabras_descr_test.join(dummies_palabras_titulo_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NtYBqnVY5tVL",
        "colab": {}
      },
      "source": [
        "import category_encoders as ce\n",
        "cat_cols = ['tipodepropiedad','ciudad', 'provincia', 'idzona']\n",
        "target_enc = ce.TargetEncoder(cols = cat_cols)\n",
        "target_enc.fit(train[cat_cols], train['publicaciones_por_año'])\n",
        "target_encoder_publicaciones = target_enc.transform(train[cat_cols]).add_suffix('_target_publicaciones')\n",
        "target_encoder_publicaciones_test = target_enc.transform(test[cat_cols]).add_suffix('_target_publicaciones')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uUjlnk7PxNgv",
        "colab": {}
      },
      "source": [
        "cat_cols = ['tipodepropiedad','ciudad', 'provincia', 'idzona']\n",
        "target_enc = ce.TargetEncoder(cols = cat_cols)\n",
        "target_enc.fit(train[cat_cols], train['metroscubiertos'])\n",
        "target_encoder_metros = target_enc.transform(train[cat_cols]).add_suffix('_target_metros')\n",
        "target_encoder_metros_test = target_enc.transform(test[cat_cols]).add_suffix('_target_metros')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-eHVS7osp6-g",
        "colab": {}
      },
      "source": [
        "Numericos=['antiguedad','habitaciones','garages','banos','metroscubiertos',\\\n",
        "            'metrostotales','idzona', 'lat', 'lng','gimnasio','usosmultiples','piscina','escuelascercanas',\\\n",
        "            'centroscomercialescercanos','puntaje_por_fecha', 'publicaciones_por_año'] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I8E2qhyKNd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_features(mexico_housing_dataframe):\n",
        " \n",
        "  selected_features = mexico_housing_dataframe[Numericos]\n",
        "  #DUmmies palabras no funciono en TF\n",
        "  selected_features = selected_features.join(target_encoder_metros).join(target_encoder_publicaciones)\n",
        "  selected_features = selected_features.rename(columns={'desc_jardín': 'desc_jardin', 'desc_salón': 'desc_salon', 'publicaciones_por_año':'publicaciones_por_anio'})\n",
        "  processed_features = selected_features.copy()\n",
        "  \n",
        "  return processed_features\n",
        "\n",
        "def preprocess_targets(data):\n",
        "  output_targets = pd.DataFrame()\n",
        "  output_targets[\"precio\"] = (\n",
        "    data[\"precio\"] / 1000.0)\n",
        "  return output_targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPnT4LZn-m0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def construct_feature_columns(input_features):\n",
        "  return set([tf.feature_column.numeric_column(my_feature)\n",
        "              for my_feature in input_features])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpECny1F-xm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
        "    features = {key:np.array(value) for key,value in dict(features).items()}                                             \n",
        " \n",
        "    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
        "    \n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(10000)\n",
        "    \n",
        "    features, labels = ds.make_one_shot_iterator().get_next()\n",
        "    return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0X4Aa7u-1jt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_nn_regression_model(\n",
        "    learning_rate,\n",
        "    steps,\n",
        "    batch_size,\n",
        "    hidden_units,\n",
        "    training_examples,\n",
        "    training_targets,\n",
        "    validation_examples,\n",
        "    validation_targets):\n",
        "  \n",
        "  periods = 10\n",
        "  steps_per_period = steps / periods\n",
        "  \n",
        "  # Creacion de  DNNRegressor.\n",
        "  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "  dnn_regressor = tf.estimator.DNNRegressor(\n",
        "      feature_columns=construct_feature_columns(training_examples),\n",
        "      hidden_units=hidden_units,\n",
        "      optimizer=my_optimizer\n",
        "  )\n",
        "  \n",
        "  training_input_fn = lambda: my_input_fn(training_examples, \n",
        "                                          training_targets[\"precio\"], \n",
        "                                          batch_size=batch_size)\n",
        "  predict_training_input_fn = lambda: my_input_fn(training_examples, \n",
        "                                                  training_targets[\"precio\"], \n",
        "                                                  num_epochs=1, \n",
        "                                                  shuffle=False,\n",
        "                                                  )\n",
        "  predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n",
        "                                                    validation_targets[\"precio\"], \n",
        "                                                    num_epochs=1, \n",
        "                                                    shuffle=False,\n",
        "                                                    )\n",
        "\n",
        "  \n",
        "  print(\"Training model...\")\n",
        "  print(\"RMSE (on training data):\")\n",
        "  training_rmse = []\n",
        "  validation_rmse = []\n",
        "  for period in range (0, periods):\n",
        "    dnn_regressor.train(\n",
        "        input_fn=training_input_fn,\n",
        "        steps=steps_per_period\n",
        "    )\n",
        "   \n",
        "    training_predictions = dnn_regressor.predict(input_fn=predict_training_input_fn)\n",
        "    training_predictions = np.array([item['predictions'][0] for item in training_predictions])\n",
        "    \n",
        "    validation_predictions = dnn_regressor.predict(input_fn=predict_validation_input_fn)\n",
        "    validation_predictions = np.array([item['predictions'][0] for item in validation_predictions])\n",
        "    \n",
        "    training_root_mean_squared_error = math.sqrt(\n",
        "        metrics.mean_squared_error(training_predictions, training_targets))\n",
        "    validation_root_mean_squared_error = math.sqrt(\n",
        "        metrics.mean_squared_error(validation_predictions, validation_targets))\n",
        "    \n",
        "    print(\"  period %02d : %0.2f\" % (period, training_root_mean_squared_error))\n",
        "    \n",
        "    training_rmse.append(training_root_mean_squared_error)\n",
        "    validation_rmse.append(validation_root_mean_squared_error)\n",
        "  print(\"Model training finished.\")\n",
        "\n",
        " \n",
        "  plt.ylabel(\"RMSE\")\n",
        "  plt.xlabel(\"Periods\")\n",
        "  plt.title(\"Root Mean Squared Error vs. Periods\")\n",
        "  plt.tight_layout()\n",
        "  plt.plot(training_rmse, label=\"training\")\n",
        "  plt.plot(validation_rmse, label=\"validation\")\n",
        "  plt.legend()\n",
        "\n",
        "  print(\"Final RMSE (on training data):   %0.2f\" % training_root_mean_squared_error)\n",
        "  print(\"Final RMSE (on validation data): %0.2f\" % validation_root_mean_squared_error)\n",
        "\n",
        "  return dnn_regressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqpNzcgYFYri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mexico_housing_dataframe = train\n",
        "mexico_housing_dataframe = mexico_housing_dataframe.reindex(\n",
        "    np.random.permutation(mexico_housing_dataframe.index))\n",
        "\n",
        "X = train[Numericos].join(dummies_palabras).join(target_encoder_metros).join(target_encoder_publicaciones)\n",
        "X_test = test[Numericos].join(dummies_palabras_test).join(target_encoder_metros_test).join(target_encoder_publicaciones_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eS-SXDaHWkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Choose the first 12000 (out of 17000) examples for training.\n",
        "training_examples = preprocess_features(mexico_housing_dataframe.head(160000))\n",
        "training_targets = preprocess_targets(mexico_housing_dataframe.head(160000))\n",
        "\n",
        "# Choose the last 5000 (out of 17000) examples for validation.\n",
        "validation_examples = preprocess_features(mexico_housing_dataframe.tail(80000))\n",
        "validation_targets = preprocess_targets(mexico_housing_dataframe.tail(80000))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHuie_NFJast",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dnn_regressor = train_nn_regression_model(\n",
        "    learning_rate=0.3,\n",
        "    steps=500,\n",
        "    batch_size=1,\n",
        "    hidden_units=[10, 10],\n",
        "    training_examples=training_examples,\n",
        "    training_targets=training_targets,\n",
        "    validation_examples=validation_examples,\n",
        "    validation_targets=validation_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZlt5vupmHS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CONFIGURACION DE INPUTS\n",
        "test[\"precio\"] = 0\n",
        "test_targets = preprocess_targets(test)\n",
        "test_features = preprocess_features(test)\n",
        "predict_test_input_fn = lambda: my_input_fn(\n",
        "      test_features, \n",
        "      test_targets[\"precio\"], \n",
        "      num_epochs=1, \n",
        "      batch_size=600,\n",
        "      shuffle=False)\n",
        "\n",
        "#EJECUTO PREDICCION\n",
        "test_predictions = linear_regressor.predict(input_fn=predict_test_input_fn)\n",
        "test_predictions = np.array([item['predictions'][0] for item in test_predictions])\n",
        "\n",
        "#CREACION DE SUBMIT\n",
        "submit = test.loc[:, ['id']]\n",
        "submit['target'] = test_predictions * 1000\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4OBfQUpmN-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DESCARGO SUBMIT\n",
        "from google.colab import files\n",
        "submit.to_csv('submit_TensorFlow_GD_RedesNeuronales_1.csv', index = False)\n",
        "files.download('submit_TensorFlow_GD_RedesNeuronales_1.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEEzZp72_n_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mexico_housing_test_data = test\n",
        "mexico_housing_test_data[\"precio\"] = 0\n",
        "test_examples = preprocess_features(mexico_housing_test_data)\n",
        "test_targets = preprocess_targets(mexico_housing_test_data)\n",
        "\n",
        "predict_testing_input_fn = lambda: my_input_fn(test_examples, \n",
        "                                               test_targets[\"precio\"], \n",
        "                                               num_epochs=1, \n",
        "                                               shuffle=False)\n",
        "\n",
        "test_predictions = nn_regressor.predict(input_fn=predict_testing_input_fn)\n",
        "test_predictions = np.array([item['predictions'][0] for item in test_predictions])\n",
        "\n",
        "root_mean_squared_error = math.sqrt(\n",
        "    metrics.mean_squared_error(test_predictions, test_targets))\n",
        "\n",
        "print(\"Final RMSE: %0.2f\" % root_mean_squared_error)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}